Architecture_Component,Subcomponent,Parameter,Value,Notes
model_rebranding,old_name,,PASE-Net (Physics-informed Attention-based Squeeze-Excitation Network),
model_rebranding,new_name,,Enhanced Attention Network (EAN) - Dual-Branch SE and Temporal Attention,
model_rebranding,reason,,Remove false physics claims while preserving attention mechanism research,
model_rebranding,experimental_evidence,,Multi-model analysis shows r=0.000±0.000 across all models,
architecture_components,total_parameters,,640713,
architecture_components,attention_parameters,,411520,
architecture_components,attention_percentage,,64.2,
architecture_components,model_size_mb,,2.44,
architecture_components,architecture_type,,CNN + SE + Temporal Attention,
component_specifications,depthwise_separable_conv,formula,x_conv = DepthwiseConv2D(x) + PointwiseConv2D(x),
component_specifications,depthwise_separable_conv,purpose,hierarchical spatial feature extraction,
component_specifications,depthwise_separable_conv,optimization,memory-constrained environments,
component_specifications,squeeze_excitation_modules,formula,x_SE = x_conv ⊗ SE(x_conv),
component_specifications,squeeze_excitation_modules,operation,element-wise multiplication,
component_specifications,squeeze_excitation_modules,purpose,channel-wise attention,
component_specifications,squeeze_excitation_modules,efficiency,minimal computational overhead,
component_specifications,temporal_self_attention,global_avg_pooling,"x_temp = GAP(x_SE, dim=F)  // [B,C,T,F] → [B,C,T]",
component_specifications,temporal_self_attention,attention_formula,"Attention(Q,K,V) = softmax(QK^T/√d_k)V",
component_specifications,temporal_self_attention,multi_head,"MultiHeadAttention(x_temp, h=4)",
component_specifications,temporal_self_attention,heads,4,
design_principles,multi_scale_feature_learning,technique,CNN layers with depthwise separable convolutions,
design_principles,multi_scale_feature_learning,benefit,hierarchical spatial features at different abstraction levels,
design_principles,multi_scale_feature_learning,optimization,compact representations for memory-constrained environments,
design_principles,efficient_channel_attention,mechanism,SE modules,
design_principles,efficient_channel_attention,benefit,selective emphasis on informative feature channels,
design_principles,efficient_channel_attention,efficiency,minimal computational overhead,
design_principles,efficient_channel_attention,application,WiFi CSI data where different frequency subcarriers have varying importance,
design_principles,temporal_dependency_modeling,mechanism,multi-head attention,
design_principles,temporal_dependency_modeling,benefit,captures long-range temporal relationships,
design_principles,temporal_dependency_modeling,efficiency,computational efficiency for real-time edge inference,
design_principles,temporal_dependency_modeling,advantage,direct modeling without gradient vanishing issues,
edge_optimization,memory_footprint,,2.44MB,
edge_optimization,gpu_speedup,,64.1x (338.91ms CPU → 5.29ms GPU),
edge_optimization,throughput,,607 samples/second,
edge_optimization,single_sample_latency,,5.3ms,
edge_optimization,deployment_status,,Production Ready,
experimental_validation,cross_domain_performance,loso_f1,83.0±0.1%,
experimental_validation,cross_domain_performance,loro_f1,83.0±0.1%,
experimental_validation,cross_domain_performance,consistency,identical performance across protocols,
experimental_validation,cross_domain_performance,coefficient_variation,<0.2%,
experimental_validation,attention_analysis,se_attention_real,extracted from 3 SE modules per Enhanced model,
experimental_validation,attention_analysis,temporal_attention_patterns,6 activities analysis,
experimental_validation,attention_analysis,statistical_testing,bootstrap confidence analysis framework,
experimental_validation,attention_analysis,physics_correlation_reality,r=0.000±0.000 across all models (NOT r=0.73),CORRECTED
corrected_interpretability,attention_mechanisms,se_channel_attention,real patterns extracted from trained models,
corrected_interpretability,attention_mechanisms,temporal_attention_dynamics,activity-specific temporal focus,
corrected_interpretability,attention_mechanisms,visualization,"2-subplot Figure 6: (a) SE Channel Attention, (b) Temporal Attention",
corrected_interpretability,removed_false_claims,physics_correlation_subplot,removed entirely from Figure 6c,CORRECTED
corrected_interpretability,removed_false_claims,false_correlation_value,"r=0.73, p<0.001 - EXPERIMENTALLY DISPROVEN",CORRECTED
corrected_interpretability,removed_false_claims,honest_reporting,attention mechanisms without false physics claims,
architectural_advantages,domain_agnostic_learning,,learns invariant properties of human-signal interactions,
architectural_advantages,cross_domain_robustness,,robust to domain-specific variations,
architectural_advantages,computational_efficiency,,suitable for edge deployment scenarios,
architectural_advantages,attention_effectiveness,,proven through systematic evaluation protocols,
